{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729c44a-901e-4f61-b167-e2c07baa7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12018fd9-0aa3-4574-9d52-3c2ac424d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to process image data, we use fastai.vision\n",
    "from fastai.vision.all import * \n",
    "# This is the latest point where we really should use the GPU for computing. \n",
    "# We first test, if there is a CUDA device available\n",
    "print( torch.cuda.get_device_name(0) )\n",
    "print( torch.__version__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73a631-4c32-4f45-a857-a919c98c2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed -> pseudo random \n",
    "torch.manual_seed(0) # for pyTorch\n",
    "random.seed(0)       # for python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81277a-544f-4c69-b77f-86d82e2f8029",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8f0c1-b60c-41e1-a4e9-cc72ce371f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# First, we create a path-object that points to the data\n",
    "path = Path('data/mnist_png/')\n",
    "sub_directories = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "     \n",
    "image_files = get_image_files(path)\n",
    "image_files\n",
    "\n",
    "# Now we define a function that creates a label for each filename. In our case,\n",
    "# the class is encoded in the directory name. To create the label function,\n",
    "# we typically have to check the directory structure and the structure of the filenames.\n",
    "def label_function(filename):\n",
    "    return filename.parents[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97574e07-6ff0-4549-8f6f-c324258178a2",
   "metadata": {},
   "source": [
    "When loading the images, we can pass transfomations to change the images (for example adapt the size) and realize data augmentations. In our case the images already have the appropriate size (64x64 pixels), but we use severeal data augmentations to improve generalization. Also, we normalize the images to the statistics from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcaf37-f80b-4e53-82b6-4a37542e534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_transforms  = []\n",
    "\n",
    "# If images need to be cropped in size, this can be done as follows:\n",
    "# A drawback of the vgg16 is the fixed input size of $244\\times244$ pixels. \n",
    "item_transforms = []\n",
    "batch_transforms = [*aug_transforms(size=224), Normalize.from_stats(*imagenet_stats)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ac17d-5625-4550-a94a-55223b038a2e",
   "metadata": {},
   "source": [
    "We create a random spliter object to split data data in training- and validation data. Test data is split on file level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf78ed-c708-4dcb-87c8-3521af9d82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RandomSplitter(valid_pct=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72097bb4-ddd6-4b24-b123-e1f5fcd3bb6a",
   "metadata": {},
   "source": [
    "Now we create a DataBlock. Hereby, we specify first which \"Blocks\", i.e. types of encoding, we want to use. In our case we need an ImageBlock for the input and a CategoryBlock for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a5564d-9bb4-4e8c-9d63-16981588602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (ImageBlock, CategoryBlock)\n",
    "\n",
    "block = DataBlock(blocks=blocks,\n",
    "                  get_items=get_image_files,\n",
    "                  get_y=label_function,\n",
    "                  splitter=splitter,\n",
    "                  item_tfms=item_transforms,\n",
    "                  batch_tfms=batch_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a00ca-27ab-4603-814e-35820cc0d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "data_loader = block.dataloaders(path, bs=batchSize, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b01098-feba-4e8e-81fa-19020f0b454f",
   "metadata": {},
   "source": [
    "To check the impact of the transformations, we can repeatedly executre the cell below. Every run creates new, transformed, images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c8ef9-1611-4863-a2ae-5a3ba9f2010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464336b1-89b2-4e7f-8045-fbd142ab2865",
   "metadata": {},
   "source": [
    "## Training: VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff3d02-27b4-4b25-b25d-c446c6297597",
   "metadata": {},
   "source": [
    "Now we know how our data looks and are convinced, that the loading of both images and labels, as well as the transformations, work as intended. It is time to train a first model. You can experiment with different architectures and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c739d5c-0b4c-4ab6-abb8-8f921ac568b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "architektur = vgg16\n",
    "metrik = error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9186197-631e-49a9-9b2b-a79b47300d97",
   "metadata": {},
   "source": [
    "Aditional architectures to test are:\n",
    "\n",
    "* alexnet\n",
    "* vgg16\n",
    "* densenet_121 (161, 169, 201)\n",
    "* resnet18 (34, 50, 101, 152)\n",
    "\n",
    "Even more architectures are in \"torchvision\". This can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09c7fb-25ff-4df8-a309-f87729f73489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as torchModels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc0433-3dbe-42a2-bb16-a5a51b50ecce",
   "metadata": {},
   "source": [
    "Additional metrics, that can be tested, include:\n",
    "* accuracy\n",
    "* error_rate\n",
    "* dice\n",
    "\n",
    "There are more metrics, which cannot be tested with our example:\n",
    "* mean_squared_error\n",
    "* mean_absolute_error\n",
    "* mean_squared_logarithmic_error\n",
    "* exp_rmspe\n",
    "* explained_variance\n",
    "* r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac29b5-a801-4baa-8916-89dd7efa2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = vision_learner(data_loader,\n",
    "                         architektur,\n",
    "                         metrics = metrik)\n",
    "\n",
    "learner.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57762b0f-c44c-4dc9-8fad-71983e390223",
   "metadata": {},
   "source": [
    "The learning rate finder automatically tests different learning rates. Denken Sie daran, den Lerner vorher neu zu erstellen, damit Sie immer mit einer zuf√§lligen Initialisierung der Gewichte starten.\n",
    "We store the model with the randomly initialized weights for later experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f4c1e-1e9d-424c-96a0-0b4fdf21995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save(\"vgg16_initial\")\n",
    "learner.lr_find()\n",
    "gewaehlteLernrate = 1e-03\n",
    "learner.load(\"vgg16_initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cfe52-1d2b-4d2c-82ce-c06edafa0772",
   "metadata": {},
   "source": [
    "The fit function trains one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156b02d-f511-4059-9447-046cdc9995e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit( 1, lr=gewaehlteLernrate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c72f36-4e36-4414-a55e-8f11d58cd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9c6ac-b718-4ca0-80a4-164ce0aedc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('vgg16_phase1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9fd811-6519-4d59-9ef3-25139b291ff7",
   "metadata": {},
   "source": [
    "## Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aefd9b-3c75-4602-a06c-baf67ce8432c",
   "metadata": {},
   "source": [
    "We can now examine the models we have learned with regard to the accuracy of the classification. First, we look at which class is most frequently interchanged with which class. Of course, this comparison makes more sense for many classes. In our case with 2 classes, we can only see whether the proportion of swaps is roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093eb43-9c17-487a-a139-0023404a1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation = ClassificationInterpretation.from_learner(learner)\n",
    "interpretation.plot_top_losses(9, figsize=(20,11))\n",
    "interpretation.most_confused(min_val=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e358c6-14f1-4bc1-ba69-d42ec715a382",
   "metadata": {},
   "source": [
    "With the help of a confusion matrix, this can also be easily seen in our simple case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7250b2-5e70-4498-b2ac-fd3a478762df",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretation.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
