{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying From Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To classify tabular data we use fastai.tabular\n",
    "from fastai.tabular.all import * \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "# Test data get partitioned into a separate file already now\n",
    "from pathlib import Path\n",
    "data_path = Path('./data/mecs/MECS_2-Phase-Steels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>equiv. diameter</th>\n",
       "      <th>major axis length</th>\n",
       "      <th>minor axis length</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>equiv. radius</th>\n",
       "      <th>max feret diameter</th>\n",
       "      <th>min feret diameter</th>\n",
       "      <th>mean feret diameter</th>\n",
       "      <th>convex perimeter</th>\n",
       "      <th>...</th>\n",
       "      <th>std. relativ area</th>\n",
       "      <th>std. convex area/filled area</th>\n",
       "      <th>std. axial ratio</th>\n",
       "      <th>std. aspect ratio</th>\n",
       "      <th>std. roundness</th>\n",
       "      <th>std. circularity</th>\n",
       "      <th>std. sphericity</th>\n",
       "      <th>std. convex per./filled per.</th>\n",
       "      <th>std. form factor</th>\n",
       "      <th>std. convexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>category_3</td>\n",
       "      <td>15.225748</td>\n",
       "      <td>11.478270</td>\n",
       "      <td>6.516211</td>\n",
       "      <td>101.586092</td>\n",
       "      <td>7.612874</td>\n",
       "      <td>28.476331</td>\n",
       "      <td>14.601479</td>\n",
       "      <td>21.538905</td>\n",
       "      <td>69.846721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.360265</td>\n",
       "      <td>0.247471</td>\n",
       "      <td>0.172865</td>\n",
       "      <td>0.167522</td>\n",
       "      <td>0.150141</td>\n",
       "      <td>0.991127</td>\n",
       "      <td>0.257917</td>\n",
       "      <td>0.254766</td>\n",
       "      <td>0.725854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>category_3</td>\n",
       "      <td>1.151113</td>\n",
       "      <td>0.974243</td>\n",
       "      <td>0.385487</td>\n",
       "      <td>5.609659</td>\n",
       "      <td>0.575557</td>\n",
       "      <td>2.580962</td>\n",
       "      <td>0.888856</td>\n",
       "      <td>1.734909</td>\n",
       "      <td>5.555157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0.345320</td>\n",
       "      <td>0.108488</td>\n",
       "      <td>0.097143</td>\n",
       "      <td>0.104520</td>\n",
       "      <td>0.107560</td>\n",
       "      <td>0.357665</td>\n",
       "      <td>0.126119</td>\n",
       "      <td>0.195196</td>\n",
       "      <td>0.270121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>category_3</td>\n",
       "      <td>1.450391</td>\n",
       "      <td>1.068283</td>\n",
       "      <td>0.524281</td>\n",
       "      <td>5.629028</td>\n",
       "      <td>0.725196</td>\n",
       "      <td>2.312959</td>\n",
       "      <td>1.144310</td>\n",
       "      <td>1.728635</td>\n",
       "      <td>5.545392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102618</td>\n",
       "      <td>0.441363</td>\n",
       "      <td>0.205836</td>\n",
       "      <td>0.167468</td>\n",
       "      <td>0.075052</td>\n",
       "      <td>0.079354</td>\n",
       "      <td>0.199565</td>\n",
       "      <td>0.134383</td>\n",
       "      <td>0.116518</td>\n",
       "      <td>0.255049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>category_3</td>\n",
       "      <td>3.561641</td>\n",
       "      <td>2.350421</td>\n",
       "      <td>1.400185</td>\n",
       "      <td>13.099335</td>\n",
       "      <td>1.780821</td>\n",
       "      <td>5.162738</td>\n",
       "      <td>3.039382</td>\n",
       "      <td>4.101060</td>\n",
       "      <td>12.632860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040841</td>\n",
       "      <td>0.347140</td>\n",
       "      <td>0.234101</td>\n",
       "      <td>0.159523</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>0.144939</td>\n",
       "      <td>1.299666</td>\n",
       "      <td>0.328635</td>\n",
       "      <td>0.253970</td>\n",
       "      <td>1.025188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category_3</td>\n",
       "      <td>6.676158</td>\n",
       "      <td>5.226249</td>\n",
       "      <td>2.784733</td>\n",
       "      <td>35.279133</td>\n",
       "      <td>3.338079</td>\n",
       "      <td>12.536480</td>\n",
       "      <td>6.907373</td>\n",
       "      <td>9.721926</td>\n",
       "      <td>30.297397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.257991</td>\n",
       "      <td>0.253047</td>\n",
       "      <td>0.162753</td>\n",
       "      <td>0.156650</td>\n",
       "      <td>0.134192</td>\n",
       "      <td>1.073103</td>\n",
       "      <td>0.273935</td>\n",
       "      <td>0.238783</td>\n",
       "      <td>0.793738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class  equiv. diameter  major axis length  minor axis length  \\\n",
       "0  category_3        15.225748          11.478270           6.516211   \n",
       "1  category_3         1.151113           0.974243           0.385487   \n",
       "2  category_3         1.450391           1.068283           0.524281   \n",
       "3  category_3         3.561641           2.350421           1.400185   \n",
       "4  category_3         6.676158           5.226249           2.784733   \n",
       "\n",
       "    perimeter  equiv. radius  max feret diameter  min feret diameter  \\\n",
       "0  101.586092       7.612874           28.476331           14.601479   \n",
       "1    5.609659       0.575557            2.580962            0.888856   \n",
       "2    5.629028       0.725196            2.312959            1.144310   \n",
       "3   13.099335       1.780821            5.162738            3.039382   \n",
       "4   35.279133       3.338079           12.536480            6.907373   \n",
       "\n",
       "   mean feret diameter  convex perimeter  ...  std. relativ area  \\\n",
       "0            21.538905         69.846721  ...           0.002630   \n",
       "1             1.734909          5.555157  ...           0.193671   \n",
       "2             1.728635          5.545392  ...           0.102618   \n",
       "3             4.101060         12.632860  ...           0.040841   \n",
       "4             9.721926         30.297397  ...           0.009716   \n",
       "\n",
       "   std. convex area/filled area  std. axial ratio  std. aspect ratio  \\\n",
       "0                      0.360265          0.247471           0.172865   \n",
       "1                      0.345320          0.108488           0.097143   \n",
       "2                      0.441363          0.205836           0.167468   \n",
       "3                      0.347140          0.234101           0.159523   \n",
       "4                      0.257991          0.253047           0.162753   \n",
       "\n",
       "   std. roundness  std. circularity  std. sphericity  \\\n",
       "0        0.167522          0.150141         0.991127   \n",
       "1        0.104520          0.107560         0.357665   \n",
       "2        0.075052          0.079354         0.199565   \n",
       "3        0.160860          0.144939         1.299666   \n",
       "4        0.156650          0.134192         1.073103   \n",
       "\n",
       "   std. convex per./filled per.  std. form factor  std. convexity  \n",
       "0                      0.257917          0.254766        0.725854  \n",
       "1                      0.126119          0.195196        0.270121  \n",
       "2                      0.134383          0.116518        0.255049  \n",
       "3                      0.328635          0.253970        1.025188  \n",
       "4                      0.273935          0.238783        0.793738  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading CSV file containing training and validation data\n",
    "import pandas as pd\n",
    "dataframe = pd.read_csv(data_path, sep=';')\n",
    "\n",
    "# Looking at the first data\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.arange(6).reshape(3,2).mean(axis=0)\n",
    "s = np.arange(6).reshape(3,2).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_features = len(dataframe.columns)-1\n",
    "\n",
    "def extract_numpy_from_df( dataframe: pd.DataFrame, y_column: int ):\n",
    "    y_column_name = dataframe.columns[y_column]\n",
    "    X = dataframe.drop(y_column_name, axis=1).to_numpy(dtype=np.float32)\n",
    "    Y = dataframe[y_column_name].to_numpy()\n",
    "    return X,Y\n",
    "\n",
    "def clean_data( X, Y ):\n",
    "    indices = ~np.isnan(X).any(axis=1)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "    return X, Y\n",
    "    \n",
    "def get_validation_and_training_indices( dataset_length ):\n",
    "    indices = np.random.permutation(dataset_length)\n",
    "    t = int(dataset_length * 0.8)\n",
    "\n",
    "    return indices[:t], indices[t:]\n",
    "\n",
    "def normalize( X ):\n",
    "    m = X.mean(axis=0)\n",
    "    s = X.std(axis=0)\n",
    "    \n",
    "    return (X - m) / s \n",
    "\n",
    "def hot_1_encode( Y, codes ):\n",
    "    Y_encoded = np.zeros((Y.shape[0], len(codes)))\n",
    "    for i, elem in enumerate(Y):\n",
    "        Y_encoded[i, codes.get(elem)] = 1\n",
    "    \n",
    "    return Y_encoded\n",
    "\n",
    "def create_batch( permutation, batch_no, batch_size, X ):\n",
    "    indices = permutation[batch_no * batch_size: batch_no * batch_size + batch_size]\n",
    "    x_batch = torch.zeros( [ len(indices), X.shape[1] ], dtype=torch.float32 )\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        x_batch[i] = torch.from_numpy(X[indices[i]])\n",
    "    return x_batch\n",
    "\n",
    "codes = { 'category_1' : 0, 'category_2' : 1, 'category_3' : 2 }\n",
    "\n",
    "X,Y = extract_numpy_from_df( dataframe, 0 )\n",
    "X,Y   = clean_data( X, Y )\n",
    "indices_train,indices_validation = get_validation_and_training_indices( X.shape[0] )\n",
    "\n",
    "X   = normalize( X )\n",
    "Y   = hot_1_encode( Y, codes )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model( n_in, n_hidden, n_out ):\n",
    "    layer = []\n",
    "    if n_hidden:\n",
    "        layer.append(nn.Linear(n_in, n_hidden[0]))\n",
    "        layer.append(nn.ReLU())\n",
    "        for i in range(len(n_hidden) - 1):\n",
    "            layer.append(nn.Linear(n_hidden[i], n_hidden[i + 1]))\n",
    "            layer.append(nn.ReLU())\n",
    "        layer.append(nn.Linear(n_hidden[-1], n_out))\n",
    "    else:\n",
    "        layer.append(nn.Linear(n_in, n_out))\n",
    "\n",
    "    model = nn.Sequential( *layer )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric( y, y_hat ):\n",
    "    y     = torch.argmax(y, dim=1).to(torch.float32)\n",
    "    y_hat = torch.argmax(y_hat, dim=1).to(torch.float32)\n",
    "    difference = y_hat-y\n",
    "    return 1.0 - torch.mean( torch.abs( difference ) ).item() \n",
    "\n",
    "def train_one_epoch( epoch_index, model, indices_train, X, Y, optimizer, loss_fn, batch_size, writer ):\n",
    "    no_train_batches = int(len(indices_train) / batch_size)\n",
    "    losses           = []\n",
    "    accuracies       = []\n",
    "\n",
    "    for batch_no in range(no_train_batches + 1):\n",
    "        x = create_batch(indices_train, batch_no, batch_size, X)\n",
    "        y = create_batch(indices_train, batch_no, batch_size, Y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy_metric(y, y_pred))\n",
    "    writer.add_scalar(\"Loss/train\", np.mean(losses), epoch_index)\n",
    "    writer.add_scalar(\"Accuracy/train\", np.mean(accuracies), epoch_index)\n",
    "    return np.mean(losses), np.mean(accuracies)\n",
    "\n",
    "\n",
    "def validate(epoch_index, model, indices_val, X, Y, loss_fn, batch_size, writer):\n",
    "    no_val_batches = int(len(indices_val) / batch_size)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for batch_no in range(no_val_batches):\n",
    "        x = create_batch(indices_val, batch_no, batch_size, X)\n",
    "        y = create_batch(indices_val, batch_no, batch_size, Y)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(accuracy_metric(y, y_pred))\n",
    "\n",
    "    writer.add_scalar(\"Loss/val\", np.mean(losses), epoch_index)\n",
    "    writer.add_scalar(\"Accuracy/val\", np.mean(accuracies), epoch_index)\n",
    "    return np.mean(losses), np.mean(accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 50; acc_train: 0.2217; acc_val: 0.5838\n",
      "Epoch 1 / 50; acc_train: 0.6267; acc_val: 0.6264\n",
      "Epoch 2 / 50; acc_train: 0.6711; acc_val: 0.6757\n",
      "Epoch 3 / 50; acc_train: 0.7494; acc_val: 0.7609\n",
      "Epoch 4 / 50; acc_train: 0.8225; acc_val: 0.8144\n",
      "Epoch 5 / 50; acc_train: 0.8542; acc_val: 0.8390\n",
      "Epoch 6 / 50; acc_train: 0.8702; acc_val: 0.8537\n",
      "Epoch 7 / 50; acc_train: 0.8819; acc_val: 0.8684\n",
      "Epoch 8 / 50; acc_train: 0.8886; acc_val: 0.8731\n",
      "Epoch 9 / 50; acc_train: 0.8930; acc_val: 0.8807\n",
      "Epoch 10 / 50; acc_train: 0.8986; acc_val: 0.8845\n",
      "Epoch 11 / 50; acc_train: 0.9028; acc_val: 0.8883\n",
      "Epoch 12 / 50; acc_train: 0.9069; acc_val: 0.8939\n",
      "Epoch 13 / 50; acc_train: 0.9111; acc_val: 0.8963\n",
      "Epoch 14 / 50; acc_train: 0.9141; acc_val: 0.9006\n",
      "Epoch 15 / 50; acc_train: 0.9180; acc_val: 0.9025\n",
      "Epoch 16 / 50; acc_train: 0.9222; acc_val: 0.9044\n",
      "Epoch 17 / 50; acc_train: 0.9260; acc_val: 0.9062\n",
      "Epoch 18 / 50; acc_train: 0.9285; acc_val: 0.9086\n",
      "Epoch 19 / 50; acc_train: 0.9311; acc_val: 0.9100\n",
      "Epoch 20 / 50; acc_train: 0.9330; acc_val: 0.9105\n",
      "Epoch 21 / 50; acc_train: 0.9334; acc_val: 0.9129\n",
      "Epoch 22 / 50; acc_train: 0.9354; acc_val: 0.9143\n",
      "Epoch 23 / 50; acc_train: 0.9370; acc_val: 0.9152\n",
      "Epoch 24 / 50; acc_train: 0.9377; acc_val: 0.9157\n",
      "Epoch 25 / 50; acc_train: 0.9381; acc_val: 0.9176\n",
      "Epoch 26 / 50; acc_train: 0.9390; acc_val: 0.9176\n",
      "Epoch 27 / 50; acc_train: 0.9395; acc_val: 0.9195\n",
      "Epoch 28 / 50; acc_train: 0.9406; acc_val: 0.9209\n",
      "Epoch 29 / 50; acc_train: 0.9423; acc_val: 0.9233\n",
      "Epoch 30 / 50; acc_train: 0.9434; acc_val: 0.9238\n",
      "Epoch 31 / 50; acc_train: 0.9450; acc_val: 0.9252\n",
      "Epoch 32 / 50; acc_train: 0.9464; acc_val: 0.9247\n",
      "Epoch 33 / 50; acc_train: 0.9471; acc_val: 0.9257\n",
      "Epoch 34 / 50; acc_train: 0.9481; acc_val: 0.9261\n",
      "Epoch 35 / 50; acc_train: 0.9492; acc_val: 0.9261\n",
      "Epoch 36 / 50; acc_train: 0.9495; acc_val: 0.9266\n",
      "Epoch 37 / 50; acc_train: 0.9502; acc_val: 0.9271\n",
      "Epoch 38 / 50; acc_train: 0.9508; acc_val: 0.9261\n",
      "Epoch 39 / 50; acc_train: 0.9513; acc_val: 0.9271\n",
      "Epoch 40 / 50; acc_train: 0.9522; acc_val: 0.9290\n",
      "Epoch 41 / 50; acc_train: 0.9530; acc_val: 0.9295\n",
      "Epoch 42 / 50; acc_train: 0.9541; acc_val: 0.9304\n",
      "Epoch 43 / 50; acc_train: 0.9542; acc_val: 0.9309\n",
      "Epoch 44 / 50; acc_train: 0.9543; acc_val: 0.9304\n",
      "Epoch 45 / 50; acc_train: 0.9545; acc_val: 0.9304\n",
      "Epoch 46 / 50; acc_train: 0.9550; acc_val: 0.9313\n",
      "Epoch 47 / 50; acc_train: 0.9551; acc_val: 0.9328\n",
      "Epoch 48 / 50; acc_train: 0.9559; acc_val: 0.9328\n",
      "Epoch 49 / 50; acc_train: 0.9563; acc_val: 0.9328\n",
      "Epoch 0 / 50; acc_train: 0.6213; acc_val: 0.8191\n",
      "Epoch 1 / 50; acc_train: 0.8679; acc_val: 0.8797\n",
      "Epoch 2 / 50; acc_train: 0.9033; acc_val: 0.8982\n",
      "Epoch 3 / 50; acc_train: 0.9181; acc_val: 0.9096\n",
      "Epoch 4 / 50; acc_train: 0.9290; acc_val: 0.9157\n",
      "Epoch 5 / 50; acc_train: 0.9351; acc_val: 0.9195\n",
      "Epoch 6 / 50; acc_train: 0.9389; acc_val: 0.9257\n",
      "Epoch 7 / 50; acc_train: 0.9440; acc_val: 0.9290\n",
      "Epoch 8 / 50; acc_train: 0.9485; acc_val: 0.9332\n",
      "Epoch 9 / 50; acc_train: 0.9504; acc_val: 0.9342\n",
      "Epoch 10 / 50; acc_train: 0.9546; acc_val: 0.9366\n",
      "Epoch 11 / 50; acc_train: 0.9563; acc_val: 0.9403\n",
      "Epoch 12 / 50; acc_train: 0.9576; acc_val: 0.9403\n",
      "Epoch 13 / 50; acc_train: 0.9597; acc_val: 0.9422\n",
      "Epoch 14 / 50; acc_train: 0.9612; acc_val: 0.9427\n",
      "Epoch 15 / 50; acc_train: 0.9628; acc_val: 0.9427\n",
      "Epoch 16 / 50; acc_train: 0.9644; acc_val: 0.9446\n",
      "Epoch 17 / 50; acc_train: 0.9658; acc_val: 0.9455\n",
      "Epoch 18 / 50; acc_train: 0.9674; acc_val: 0.9446\n",
      "Epoch 19 / 50; acc_train: 0.9685; acc_val: 0.9451\n",
      "Epoch 20 / 50; acc_train: 0.9699; acc_val: 0.9474\n",
      "Epoch 21 / 50; acc_train: 0.9705; acc_val: 0.9484\n",
      "Epoch 22 / 50; acc_train: 0.9719; acc_val: 0.9489\n",
      "Epoch 23 / 50; acc_train: 0.9731; acc_val: 0.9503\n",
      "Epoch 24 / 50; acc_train: 0.9738; acc_val: 0.9512\n",
      "Epoch 25 / 50; acc_train: 0.9743; acc_val: 0.9508\n",
      "Epoch 26 / 50; acc_train: 0.9754; acc_val: 0.9508\n",
      "Epoch 27 / 50; acc_train: 0.9764; acc_val: 0.9498\n",
      "Epoch 28 / 50; acc_train: 0.9774; acc_val: 0.9498\n",
      "Epoch 29 / 50; acc_train: 0.9778; acc_val: 0.9498\n",
      "Epoch 30 / 50; acc_train: 0.9782; acc_val: 0.9493\n",
      "Epoch 31 / 50; acc_train: 0.9797; acc_val: 0.9489\n",
      "Epoch 32 / 50; acc_train: 0.9801; acc_val: 0.9503\n",
      "Epoch 33 / 50; acc_train: 0.9808; acc_val: 0.9503\n",
      "Epoch 34 / 50; acc_train: 0.9808; acc_val: 0.9493\n",
      "Epoch 35 / 50; acc_train: 0.9823; acc_val: 0.9512\n",
      "Epoch 36 / 50; acc_train: 0.9829; acc_val: 0.9503\n",
      "Epoch 37 / 50; acc_train: 0.9837; acc_val: 0.9512\n",
      "Epoch 38 / 50; acc_train: 0.9845; acc_val: 0.9512\n",
      "Epoch 39 / 50; acc_train: 0.9841; acc_val: 0.9517\n",
      "Epoch 40 / 50; acc_train: 0.9851; acc_val: 0.9512\n",
      "Epoch 41 / 50; acc_train: 0.9854; acc_val: 0.9517\n",
      "Epoch 42 / 50; acc_train: 0.9864; acc_val: 0.9512\n",
      "Epoch 43 / 50; acc_train: 0.9867; acc_val: 0.9527\n",
      "Epoch 44 / 50; acc_train: 0.9873; acc_val: 0.9522\n",
      "Epoch 45 / 50; acc_train: 0.9883; acc_val: 0.9531\n",
      "Epoch 46 / 50; acc_train: 0.9886; acc_val: 0.9555\n",
      "Epoch 47 / 50; acc_train: 0.9888; acc_val: 0.9545\n",
      "Epoch 48 / 50; acc_train: 0.9892; acc_val: 0.9560\n",
      "Epoch 49 / 50; acc_train: 0.9896; acc_val: 0.9564\n",
      "Epoch 0 / 50; acc_train: 0.7635; acc_val: 0.8741\n",
      "Epoch 1 / 50; acc_train: 0.9037; acc_val: 0.9010\n",
      "Epoch 2 / 50; acc_train: 0.9236; acc_val: 0.9134\n",
      "Epoch 3 / 50; acc_train: 0.9381; acc_val: 0.9247\n",
      "Epoch 4 / 50; acc_train: 0.9473; acc_val: 0.9290\n",
      "Epoch 5 / 50; acc_train: 0.9515; acc_val: 0.9313\n",
      "Epoch 6 / 50; acc_train: 0.9556; acc_val: 0.9313\n",
      "Epoch 7 / 50; acc_train: 0.9581; acc_val: 0.9332\n",
      "Epoch 8 / 50; acc_train: 0.9606; acc_val: 0.9328\n",
      "Epoch 9 / 50; acc_train: 0.9634; acc_val: 0.9342\n",
      "Epoch 10 / 50; acc_train: 0.9650; acc_val: 0.9351\n",
      "Epoch 11 / 50; acc_train: 0.9677; acc_val: 0.9356\n",
      "Epoch 12 / 50; acc_train: 0.9684; acc_val: 0.9370\n",
      "Epoch 13 / 50; acc_train: 0.9705; acc_val: 0.9384\n",
      "Epoch 14 / 50; acc_train: 0.9732; acc_val: 0.9384\n",
      "Epoch 15 / 50; acc_train: 0.9742; acc_val: 0.9437\n",
      "Epoch 16 / 50; acc_train: 0.9756; acc_val: 0.9451\n",
      "Epoch 17 / 50; acc_train: 0.9769; acc_val: 0.9451\n",
      "Epoch 18 / 50; acc_train: 0.9782; acc_val: 0.9446\n",
      "Epoch 19 / 50; acc_train: 0.9798; acc_val: 0.9470\n",
      "Epoch 20 / 50; acc_train: 0.9815; acc_val: 0.9465\n",
      "Epoch 21 / 50; acc_train: 0.9820; acc_val: 0.9470\n",
      "Epoch 22 / 50; acc_train: 0.9827; acc_val: 0.9455\n",
      "Epoch 23 / 50; acc_train: 0.9827; acc_val: 0.9427\n",
      "Epoch 24 / 50; acc_train: 0.9844; acc_val: 0.9465\n",
      "Epoch 25 / 50; acc_train: 0.9854; acc_val: 0.9465\n",
      "Epoch 26 / 50; acc_train: 0.9858; acc_val: 0.9460\n",
      "Epoch 27 / 50; acc_train: 0.9864; acc_val: 0.9465\n",
      "Epoch 28 / 50; acc_train: 0.9872; acc_val: 0.9441\n",
      "Epoch 29 / 50; acc_train: 0.9875; acc_val: 0.9446\n",
      "Epoch 30 / 50; acc_train: 0.9885; acc_val: 0.9446\n",
      "Epoch 31 / 50; acc_train: 0.9893; acc_val: 0.9437\n",
      "Epoch 32 / 50; acc_train: 0.9899; acc_val: 0.9437\n",
      "Epoch 33 / 50; acc_train: 0.9904; acc_val: 0.9441\n",
      "Epoch 34 / 50; acc_train: 0.9911; acc_val: 0.9437\n",
      "Epoch 35 / 50; acc_train: 0.9904; acc_val: 0.9465\n",
      "Epoch 36 / 50; acc_train: 0.9902; acc_val: 0.9437\n",
      "Epoch 37 / 50; acc_train: 0.9894; acc_val: 0.9441\n",
      "Epoch 38 / 50; acc_train: 0.9876; acc_val: 0.9484\n",
      "Epoch 39 / 50; acc_train: 0.9915; acc_val: 0.9460\n",
      "Epoch 40 / 50; acc_train: 0.9932; acc_val: 0.9460\n",
      "Epoch 41 / 50; acc_train: 0.9937; acc_val: 0.9451\n",
      "Epoch 42 / 50; acc_train: 0.9939; acc_val: 0.9451\n",
      "Epoch 43 / 50; acc_train: 0.9943; acc_val: 0.9455\n",
      "Epoch 44 / 50; acc_train: 0.9950; acc_val: 0.9465\n",
      "Epoch 45 / 50; acc_train: 0.9950; acc_val: 0.9437\n",
      "Epoch 46 / 50; acc_train: 0.9943; acc_val: 0.9446\n",
      "Epoch 47 / 50; acc_train: 0.9907; acc_val: 0.9446\n",
      "Epoch 48 / 50; acc_train: 0.9897; acc_val: 0.9418\n",
      "Epoch 49 / 50; acc_train: 0.9927; acc_val: 0.9451\n",
      "Epoch 0 / 50; acc_train: 0.8534; acc_val: 0.9115\n",
      "Epoch 1 / 50; acc_train: 0.9353; acc_val: 0.9304\n",
      "Epoch 2 / 50; acc_train: 0.9457; acc_val: 0.9323\n",
      "Epoch 3 / 50; acc_train: 0.9487; acc_val: 0.9370\n",
      "Epoch 4 / 50; acc_train: 0.9548; acc_val: 0.9389\n",
      "Epoch 5 / 50; acc_train: 0.9565; acc_val: 0.9389\n",
      "Epoch 6 / 50; acc_train: 0.9607; acc_val: 0.9418\n",
      "Epoch 7 / 50; acc_train: 0.9649; acc_val: 0.9399\n",
      "Epoch 8 / 50; acc_train: 0.9618; acc_val: 0.9384\n",
      "Epoch 9 / 50; acc_train: 0.9640; acc_val: 0.9295\n",
      "Epoch 10 / 50; acc_train: 0.9633; acc_val: 0.9432\n",
      "Epoch 11 / 50; acc_train: 0.9653; acc_val: 0.9432\n",
      "Epoch 12 / 50; acc_train: 0.9706; acc_val: 0.9427\n",
      "Epoch 13 / 50; acc_train: 0.9697; acc_val: 0.9455\n",
      "Epoch 14 / 50; acc_train: 0.9748; acc_val: 0.9441\n",
      "Epoch 15 / 50; acc_train: 0.9745; acc_val: 0.9408\n",
      "Epoch 16 / 50; acc_train: 0.9726; acc_val: 0.9356\n",
      "Epoch 17 / 50; acc_train: 0.9761; acc_val: 0.9413\n",
      "Epoch 18 / 50; acc_train: 0.9736; acc_val: 0.9427\n",
      "Epoch 19 / 50; acc_train: 0.9753; acc_val: 0.9422\n",
      "Epoch 20 / 50; acc_train: 0.9738; acc_val: 0.9432\n",
      "Epoch 21 / 50; acc_train: 0.9756; acc_val: 0.9323\n",
      "Epoch 22 / 50; acc_train: 0.9768; acc_val: 0.9347\n",
      "Epoch 23 / 50; acc_train: 0.9769; acc_val: 0.9437\n",
      "Epoch 24 / 50; acc_train: 0.9825; acc_val: 0.9427\n",
      "Epoch 25 / 50; acc_train: 0.9815; acc_val: 0.9437\n",
      "Epoch 26 / 50; acc_train: 0.9819; acc_val: 0.9465\n",
      "Epoch 27 / 50; acc_train: 0.9810; acc_val: 0.9427\n",
      "Epoch 28 / 50; acc_train: 0.9811; acc_val: 0.9437\n",
      "Epoch 29 / 50; acc_train: 0.9797; acc_val: 0.9394\n",
      "Epoch 30 / 50; acc_train: 0.9839; acc_val: 0.9451\n",
      "Epoch 31 / 50; acc_train: 0.9848; acc_val: 0.9437\n",
      "Epoch 32 / 50; acc_train: 0.9847; acc_val: 0.9455\n",
      "Epoch 33 / 50; acc_train: 0.9829; acc_val: 0.9455\n",
      "Epoch 34 / 50; acc_train: 0.9858; acc_val: 0.9399\n",
      "Epoch 35 / 50; acc_train: 0.9813; acc_val: 0.9441\n",
      "Epoch 36 / 50; acc_train: 0.9841; acc_val: 0.9366\n",
      "Epoch 37 / 50; acc_train: 0.9864; acc_val: 0.9441\n",
      "Epoch 38 / 50; acc_train: 0.9873; acc_val: 0.9437\n",
      "Epoch 39 / 50; acc_train: 0.9810; acc_val: 0.9384\n",
      "Epoch 40 / 50; acc_train: 0.9852; acc_val: 0.9451\n",
      "Epoch 41 / 50; acc_train: 0.9886; acc_val: 0.9427\n",
      "Epoch 42 / 50; acc_train: 0.9888; acc_val: 0.9427\n",
      "Epoch 43 / 50; acc_train: 0.9886; acc_val: 0.9422\n",
      "Epoch 44 / 50; acc_train: 0.9888; acc_val: 0.9446\n",
      "Epoch 45 / 50; acc_train: 0.9899; acc_val: 0.9422\n",
      "Epoch 46 / 50; acc_train: 0.9888; acc_val: 0.9295\n",
      "Epoch 47 / 50; acc_train: 0.9818; acc_val: 0.9328\n",
      "Epoch 48 / 50; acc_train: 0.9833; acc_val: 0.9427\n",
      "Epoch 49 / 50; acc_train: 0.9908; acc_val: 0.9427\n",
      "Epoch 0 / 50; acc_train: 0.8629; acc_val: 0.9115\n",
      "Epoch 1 / 50; acc_train: 0.9312; acc_val: 0.9205\n",
      "Epoch 2 / 50; acc_train: 0.9394; acc_val: 0.9351\n",
      "Epoch 3 / 50; acc_train: 0.9506; acc_val: 0.9370\n",
      "Epoch 4 / 50; acc_train: 0.9514; acc_val: 0.9323\n",
      "Epoch 5 / 50; acc_train: 0.9560; acc_val: 0.9370\n",
      "Epoch 6 / 50; acc_train: 0.9546; acc_val: 0.9361\n",
      "Epoch 7 / 50; acc_train: 0.9562; acc_val: 0.9384\n",
      "Epoch 8 / 50; acc_train: 0.9574; acc_val: 0.9446\n",
      "Epoch 9 / 50; acc_train: 0.9625; acc_val: 0.9389\n",
      "Epoch 10 / 50; acc_train: 0.9643; acc_val: 0.9446\n",
      "Epoch 11 / 50; acc_train: 0.9656; acc_val: 0.9441\n",
      "Epoch 12 / 50; acc_train: 0.9553; acc_val: 0.9408\n",
      "Epoch 13 / 50; acc_train: 0.9612; acc_val: 0.9413\n",
      "Epoch 14 / 50; acc_train: 0.9635; acc_val: 0.9451\n",
      "Epoch 15 / 50; acc_train: 0.9635; acc_val: 0.9403\n",
      "Epoch 16 / 50; acc_train: 0.9670; acc_val: 0.9418\n",
      "Epoch 17 / 50; acc_train: 0.9698; acc_val: 0.9427\n",
      "Epoch 18 / 50; acc_train: 0.9705; acc_val: 0.9484\n",
      "Epoch 19 / 50; acc_train: 0.9714; acc_val: 0.9460\n",
      "Epoch 20 / 50; acc_train: 0.9712; acc_val: 0.9418\n",
      "Epoch 21 / 50; acc_train: 0.9705; acc_val: 0.9455\n",
      "Epoch 22 / 50; acc_train: 0.9704; acc_val: 0.9432\n",
      "Epoch 23 / 50; acc_train: 0.9721; acc_val: 0.9489\n",
      "Epoch 24 / 50; acc_train: 0.9733; acc_val: 0.9474\n",
      "Epoch 25 / 50; acc_train: 0.9717; acc_val: 0.9384\n",
      "Epoch 26 / 50; acc_train: 0.9705; acc_val: 0.9403\n",
      "Epoch 27 / 50; acc_train: 0.9741; acc_val: 0.9474\n",
      "Epoch 28 / 50; acc_train: 0.9753; acc_val: 0.9493\n",
      "Epoch 29 / 50; acc_train: 0.9690; acc_val: 0.9394\n",
      "Epoch 30 / 50; acc_train: 0.9757; acc_val: 0.9465\n",
      "Epoch 31 / 50; acc_train: 0.9789; acc_val: 0.9474\n",
      "Epoch 32 / 50; acc_train: 0.9804; acc_val: 0.9474\n",
      "Epoch 33 / 50; acc_train: 0.9825; acc_val: 0.9413\n",
      "Epoch 34 / 50; acc_train: 0.9813; acc_val: 0.9437\n",
      "Epoch 35 / 50; acc_train: 0.9757; acc_val: 0.9455\n",
      "Epoch 36 / 50; acc_train: 0.9785; acc_val: 0.9474\n",
      "Epoch 37 / 50; acc_train: 0.9813; acc_val: 0.9474\n",
      "Epoch 38 / 50; acc_train: 0.9816; acc_val: 0.9418\n",
      "Epoch 39 / 50; acc_train: 0.9747; acc_val: 0.9479\n",
      "Epoch 40 / 50; acc_train: 0.9787; acc_val: 0.9455\n",
      "Epoch 41 / 50; acc_train: 0.9820; acc_val: 0.9517\n",
      "Epoch 42 / 50; acc_train: 0.9780; acc_val: 0.9465\n",
      "Epoch 43 / 50; acc_train: 0.9817; acc_val: 0.9470\n",
      "Epoch 44 / 50; acc_train: 0.9830; acc_val: 0.9498\n",
      "Epoch 45 / 50; acc_train: 0.9808; acc_val: 0.9479\n",
      "Epoch 46 / 50; acc_train: 0.9761; acc_val: 0.9522\n",
      "Epoch 47 / 50; acc_train: 0.9838; acc_val: 0.9503\n",
      "Epoch 48 / 50; acc_train: 0.9854; acc_val: 0.9503\n",
      "Epoch 49 / 50; acc_train: 0.9790; acc_val: 0.9309\n",
      "Epoch 0 / 50; acc_train: 0.7892; acc_val: 0.8854\n",
      "Epoch 1 / 50; acc_train: 0.8721; acc_val: 0.8362\n",
      "Epoch 2 / 50; acc_train: 0.8404; acc_val: 0.7519\n",
      "Epoch 3 / 50; acc_train: 0.8617; acc_val: 0.8674\n",
      "Epoch 4 / 50; acc_train: 0.8970; acc_val: 0.8764\n",
      "Epoch 5 / 50; acc_train: 0.8836; acc_val: 0.8821\n",
      "Epoch 6 / 50; acc_train: 0.9141; acc_val: 0.9015\n",
      "Epoch 7 / 50; acc_train: 0.8379; acc_val: 0.7884\n",
      "Epoch 8 / 50; acc_train: 0.8259; acc_val: 0.8125\n",
      "Epoch 9 / 50; acc_train: 0.8007; acc_val: 0.5331\n",
      "Epoch 10 / 50; acc_train: 0.7573; acc_val: 0.7836\n",
      "Epoch 11 / 50; acc_train: 0.8559; acc_val: 0.8580\n",
      "Epoch 12 / 50; acc_train: 0.9087; acc_val: 0.8982\n",
      "Epoch 13 / 50; acc_train: 0.9219; acc_val: 0.9105\n",
      "Epoch 14 / 50; acc_train: 0.9276; acc_val: 0.9062\n",
      "Epoch 15 / 50; acc_train: 0.9256; acc_val: 0.9058\n",
      "Epoch 16 / 50; acc_train: 0.8828; acc_val: 0.6383\n",
      "Epoch 17 / 50; acc_train: 0.5648; acc_val: 0.5881\n",
      "Epoch 18 / 50; acc_train: 0.5183; acc_val: 0.5156\n",
      "Epoch 19 / 50; acc_train: 0.6410; acc_val: 0.6648\n",
      "Epoch 20 / 50; acc_train: 0.6868; acc_val: 0.6742\n",
      "Epoch 21 / 50; acc_train: 0.6927; acc_val: 0.6955\n",
      "Epoch 22 / 50; acc_train: 0.7034; acc_val: 0.6970\n",
      "Epoch 23 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 24 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 25 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 26 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 27 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 28 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 29 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 30 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 31 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 32 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 33 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 34 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 35 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 36 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 37 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 38 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 39 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 40 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 41 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 42 / 50; acc_train: 0.7036; acc_val: 0.6970\n",
      "Epoch 43 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 44 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 45 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 46 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 47 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 48 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 49 / 50; acc_train: 0.7036; acc_val: 0.6974\n",
      "Epoch 0 / 50; acc_train: 0.5278; acc_val: 0.5767\n",
      "Epoch 1 / 50; acc_train: 0.5010; acc_val: 0.5014\n",
      "Epoch 2 / 50; acc_train: 0.4949; acc_val: 0.5014\n",
      "Epoch 3 / 50; acc_train: 0.4703; acc_val: 0.5014\n",
      "Epoch 4 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 5 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 6 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 7 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 8 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 9 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 10 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 11 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 12 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 13 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 14 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 15 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 16 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 17 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 18 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 19 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 20 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 21 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 22 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 23 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 24 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 25 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 26 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 27 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 28 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 29 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 30 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 31 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 32 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 33 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 34 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 35 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 36 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 37 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 38 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 39 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 40 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 41 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 42 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 43 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 44 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 45 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 46 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 47 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 48 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 49 / 50; acc_train: 0.5111; acc_val: 0.5014\n",
      "Epoch 0 / 50; acc_train: 0.4811; acc_val: 0.5014\n",
      "Epoch 1 / 50; acc_train: 0.4768; acc_val: 0.5014\n",
      "Epoch 2 / 50; acc_train: 0.4728; acc_val: 0.5014\n",
      "Epoch 3 / 50; acc_train: 0.4745; acc_val: 0.5014\n",
      "Epoch 4 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 5 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 6 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 7 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 8 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 9 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 10 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 11 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 12 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 13 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 14 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 15 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 16 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 17 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 18 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 19 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 20 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 21 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 22 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 23 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 24 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 25 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 26 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 27 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 28 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 29 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 30 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 31 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 32 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 33 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 34 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 35 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 36 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 37 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 38 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 39 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 40 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 41 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 42 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 43 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 44 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 45 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 46 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 47 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 48 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 49 / 50; acc_train: 0.4726; acc_val: 0.5014\n",
      "Epoch 0 / 50; acc_train: 0.4503; acc_val: 0.5014\n",
      "Epoch 1 / 50; acc_train: 0.4616; acc_val: 0.5014\n",
      "Epoch 2 / 50; acc_train: 0.4591; acc_val: 0.5014\n",
      "Epoch 3 / 50; acc_train: 0.4581; acc_val: 0.5014\n",
      "Epoch 4 / 50; acc_train: 0.4584; acc_val: 0.5014\n",
      "Epoch 5 / 50; acc_train: 0.4605; acc_val: 0.5014\n",
      "Epoch 6 / 50; acc_train: 0.4605; acc_val: 0.5014\n",
      "Epoch 7 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 8 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 9 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 10 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 11 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 12 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 13 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 14 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 15 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 16 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 17 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 18 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 19 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 20 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 21 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 22 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 23 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 24 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 25 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 26 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 27 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 28 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 29 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 30 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 31 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 32 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 33 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 34 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 35 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 36 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 37 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 38 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 39 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 40 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 41 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 42 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 43 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 44 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 45 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 46 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 47 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 48 / 50; acc_train: 0.4549; acc_val: 0.5014\n",
      "Epoch 49 / 50; acc_train: 0.4549; acc_val: 0.5014\n"
     ]
    }
   ],
   "source": [
    "for lr in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    # for batch_size in [32, 64, 128, 256]:\n",
    "    for batch_size in [64]:\n",
    "        model = create_model(X.shape[1], [50, 25], Y.shape[1])\n",
    "        optimizer = torch.optim.Adam( params = model.parameters(), lr=lr )\n",
    "        loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "        writer = SummaryWriter(comment=f\"lr_{lr}_bs_{batch_size}_default\")\n",
    "\n",
    "        losses_train           = []\n",
    "        accuracies_train       = []\n",
    "        losses_val           = []\n",
    "        accuracies_val       = []\n",
    "        n_epochs = 50\n",
    "        for i in range (n_epochs):\n",
    "            loss_train, acc_train = train_one_epoch( i, model, indices_train, X, Y, optimizer, loss_fn, batch_size, writer )\n",
    "            loss_val, acc_val = validate(i, model, indices_validation, X, Y, loss_fn, batch_size, writer)\n",
    "            losses_train.append(loss_train)\n",
    "            accuracies_train.append(acc_train)\n",
    "            losses_val.append(loss_val)\n",
    "            accuracies_val.append(acc_val)\n",
    "            print(f\"Epoch {i} / {n_epochs}; acc_train: {accuracies_train[-1]:.4f}; acc_val: {accuracies_val[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"./data/iris_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = { 'Iris-setosa' : 0, 'Iris-versicolor' : 1, 'Iris-virginica' : 2 }\n",
    "\n",
    "X, Y = extract_numpy_from_df( dataframe, 4 )\n",
    "indices_train,indices_validation = get_validation_and_training_indices( X.shape[0] )\n",
    "\n",
    "X   = normalize( X )\n",
    "Y   = hot_1_encode( Y, codes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 50; acc_train: 0.5000; acc_val: 0.6071\n",
      "Epoch 1 / 50; acc_train: 0.6724; acc_val: 0.7500\n",
      "Epoch 2 / 50; acc_train: 0.7069; acc_val: 0.8571\n",
      "Epoch 3 / 50; acc_train: 0.7586; acc_val: 0.8929\n",
      "Epoch 4 / 50; acc_train: 0.7845; acc_val: 0.8929\n",
      "Epoch 5 / 50; acc_train: 0.8017; acc_val: 0.9286\n",
      "Epoch 6 / 50; acc_train: 0.8190; acc_val: 0.9643\n",
      "Epoch 7 / 50; acc_train: 0.8362; acc_val: 0.9643\n",
      "Epoch 8 / 50; acc_train: 0.8707; acc_val: 0.9643\n",
      "Epoch 9 / 50; acc_train: 0.8793; acc_val: 0.9643\n",
      "Epoch 10 / 50; acc_train: 0.8707; acc_val: 0.9643\n",
      "Epoch 11 / 50; acc_train: 0.8707; acc_val: 0.9643\n",
      "Epoch 12 / 50; acc_train: 0.8879; acc_val: 0.9643\n",
      "Epoch 13 / 50; acc_train: 0.9138; acc_val: 0.9643\n",
      "Epoch 14 / 50; acc_train: 0.9138; acc_val: 0.9643\n",
      "Epoch 15 / 50; acc_train: 0.9310; acc_val: 0.9643\n",
      "Epoch 16 / 50; acc_train: 0.9310; acc_val: 0.9643\n",
      "Epoch 17 / 50; acc_train: 0.9569; acc_val: 0.9643\n",
      "Epoch 18 / 50; acc_train: 0.9655; acc_val: 0.9643\n",
      "Epoch 19 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 20 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 21 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 22 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 23 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 24 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 25 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 26 / 50; acc_train: 0.9741; acc_val: 0.9643\n",
      "Epoch 27 / 50; acc_train: 0.9741; acc_val: 0.9286\n",
      "Epoch 28 / 50; acc_train: 0.9741; acc_val: 0.9286\n",
      "Epoch 29 / 50; acc_train: 0.9741; acc_val: 0.9286\n",
      "Epoch 30 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 31 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 32 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 33 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 34 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 35 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 36 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 37 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 38 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 39 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 40 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 41 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 42 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 43 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 44 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 45 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 46 / 50; acc_train: 0.9828; acc_val: 0.9286\n",
      "Epoch 47 / 50; acc_train: 0.9914; acc_val: 0.9286\n",
      "Epoch 48 / 50; acc_train: 0.9914; acc_val: 0.9286\n",
      "Epoch 49 / 50; acc_train: 0.9914; acc_val: 0.9286\n"
     ]
    }
   ],
   "source": [
    "model = create_model(X.shape[1], [50, 25], Y.shape[1])\n",
    "optimizer = torch.optim.Adam( params = model.parameters(), lr=0.0005 )\n",
    "loss_fn   = torch.nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment=f\"iris_lr_{lr}_bs_{4}_default\")\n",
    "\n",
    "losses_train           = []\n",
    "accuracies_train       = []\n",
    "losses_val           = []\n",
    "accuracies_val       = []\n",
    "n_epochs = 50\n",
    "for i in range (n_epochs):\n",
    "    loss_train, acc_train = train_one_epoch( i, model, indices_train, X, Y, optimizer, loss_fn, 4, writer )\n",
    "    loss_val, acc_val = validate(i, model, indices_validation, X, Y, loss_fn, 4, writer)\n",
    "    losses_train.append(loss_train)\n",
    "    accuracies_train.append(acc_train)\n",
    "    losses_val.append(loss_val)\n",
    "    accuracies_val.append(acc_val)\n",
    "    print(f\"Epoch {i} / {n_epochs}; acc_train: {accuracies_train[-1]:.4f}; acc_val: {accuracies_val[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801]"
  },
  "vscode": {
   "interpreter": {
    "hash": "10df0fef79a7de769e0119fbe804c8595274fe789e8ca8f0cb1e26a2ae9b551e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
